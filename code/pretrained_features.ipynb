{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0fc3a6-9d88-4753-8045-41d600b1b23a",
   "metadata": {},
   "source": [
    "# Generating features from images with a deep network\n",
    "\n",
    "The goal of this notebook is to generate (meaningful) feature vectors from raw images. These features vectors will be easier to handle than raw images with the methods seen in the course.\n",
    "To generate these features, we will use a pre-trained Convolutional Neural Network (CNN). We will not train it, we will just use it as a feature generator. Since it has already been trained on images, it should be able to extract meaningful features from them, that should help for other different image analysis tasks.\n",
    "\n",
    "The pre-trained CNN available in Tensorflow have been trained on a particular task (classification of images into precise categories) and we want to use them for another task (classification in other categories). The idea is to get rid of the original output layer(s) of the CNN and collect the output of one of the inner layer. This layer should contain meaningful features.\n",
    "\n",
    "As this is becoming a standard process, Tensorflow has made things simple for loading a pre-trained model and getting its inner layer features. This notebook shows how this can be done.\n",
    "\n",
    "This notebook is inspired from this [Google Colab example](https://www.tensorflow.org/hub/tutorials/tf2_image_retraining). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1cae1-03c6-4a5f-9cc7-3b0bff05fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Modules to install\n",
    "# !pip install tensorflow\n",
    "# !pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffdfa14-4656-41e3-a070-3081950fd676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Hub version:\", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c894c55-50a8-45e2-ac8f-b5bf08ae5057",
   "metadata": {},
   "source": [
    "## Load the dataset and make a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67bed5-1f3f-46c2-9a7f-bde29456400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_dir = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b952f-be94-4736-a1ee-077b57ba7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "module_selection = (\"mobilenet_v2_100_224\", 224)# Others to try: (\"inception_v3\", 299), (\"resnet_v2_50\", 299)\n",
    "handle_base, pixels = module_selection\n",
    "MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/{}/feature_vector/4\".format(handle_base)\n",
    "IMAGE_SIZE = (pixels, pixels) # input size of the neural network\n",
    "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca54421-6025-4fe6-b1cc-ef10352248b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training and validation sets\n",
    "SEED = 123 # This seed will make the random split of images between train and test set fixed\n",
    "BATCH_SIZE = 32 # group images in batches of BATCH_SIZE\n",
    "\n",
    "dataflow_kwargs = dict(interpolation=\"bilinear\",  validation_split=.20,\n",
    "                       seed=SEED, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE)\n",
    "# to change labels from integers to one-hot vectors add: label_mode=\"categorical\"\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)\n",
    "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir, subset=\"validation\", shuffle=True, **dataflow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847393b6-2d1e-4d87-bad3-541e2e073260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print('Data classes:',class_names)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1): # take a batch\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n",
    "print('Image size:',images[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c5d8f-fe73-4956-a031-5fae8fcde97e",
   "metadata": {},
   "source": [
    "## Create the neural network\n",
    "To use the pre-trained model, we need to adapt the new images to the format expected by the pre-trained model. The images need to be resizes and normalized (pixel values between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ec923-61c2-4342-b080-b7a8efe93bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a model with the pre-trained part. We add at the end a dense layer (not mandatory for obtaining the features)\n",
    "print(\"Building model with\", MODULE_HANDLE)\n",
    "model = tf.keras.Sequential([\n",
    "    #----- Input formating and preprocessing\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    #----- the pre-trained model loaded with a single line:\n",
    "    hub.KerasLayer(MODULE_HANDLE, trainable=False, name='embeddings'),\n",
    "    #----- some additional layers to perform a classification task afterwards (untrained, to be trained)\n",
    "    #tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(num_classes,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "\n",
    "model.build((None,)+IMAGE_SIZE+(3,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163a683-18ad-496c-ae37-dc07781560e8",
   "metadata": {},
   "source": [
    "In order to collect the output of an inner layer we create an intermediate model. We could have omitted the last layer in the previous definition, but to be more general we define a complete model and then show how to extract a part of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e7d20-d97a-4578-ad10-774176df2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the intermediate model that output features\n",
    "layer_name='embeddings'\n",
    "layer_output=model.get_layer(layer_name).output\n",
    "intermediate_model=tf.keras.models.Model(inputs=model.input, outputs=layer_output)\n",
    "embeddings_size = intermediate_model.output.shape[1]\n",
    "print('Embeddings vector size (output of the intermediate NN):',embeddings_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b7f67-da67-4220-8c65-4a8e8f188c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us collect the embeddings by iterating over batches.\n",
    "embeddings = None\n",
    "labels_vector = None\n",
    "for images, labels in valid_ds:\n",
    "    embeddings_batch = intermediate_model.predict(images)\n",
    "    if embeddings is None:\n",
    "        embeddings = embeddings_batch\n",
    "        labels_vector = labels.numpy() # convert tensors to numpy arrays\n",
    "    else:\n",
    "        embeddings = np.vstack((embeddings, embeddings_batch))\n",
    "        labels_vector = np.hstack((labels_vector, labels))\n",
    "print('Embeddings shape:', embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1784f0-1631-4199-996a-2d7096abbd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(embeddings)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Samples')\n",
    "plt.title('Feature values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4e911-9404-4e41-8bd9-872d531eba1b",
   "metadata": {},
   "source": [
    "The features are positive because there a going out of a relu function. Also, many of them are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59680a87-167f-4213-8e41-1de75ce51e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset and labels\n",
    "np.savez_compressed('dataPatternRecognition.npz', embeddings=embeddings, labels_vector=labels_vector, class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e336bb-87ad-4425-ad96-256fae6f81b9",
   "metadata": {},
   "source": [
    "# Training the full model (Not required for the course)\n",
    "This is just a demo of NN training in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558beb2-8092-437e-a9c9-6918bfc0904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), \n",
    "    #loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db9583-3644-483a-8ed9-3579ea7f539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_ds, validation_data=valid_ds, epochs=4).history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf8700-a3ec-45db-9e88-5a7383c794da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "#plt.ylim([0,2])\n",
    "plt.plot(hist[\"loss\"])\n",
    "plt.plot(hist[\"val_loss\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "#plt.ylim([0,1])\n",
    "plt.plot(hist[\"accuracy\"])\n",
    "plt.plot(hist[\"val_accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a423c7-fe04-4246-8b61-4b1c21cb24a4",
   "metadata": {},
   "source": [
    "## Visualizing the results of the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81519eb-0e7f-4e0e-86a0-18e60d86942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "for images, labels in valid_ds.take(1):\n",
    "    predictions = model.predict(images)\n",
    "    for i in range(30):\n",
    "        ax = plt.subplot(5, 6, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        pred_i = class_names[np.argmax(predictions[i])]\n",
    "        color = \"green\" if pred_i == class_names[labels[i]] else \"red\"\n",
    "        plt.title(class_names[labels[i]]+' Classified: '+pred_i, color=color)\n",
    "        #plt.title(class_names[np.argmax(pred[i])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82ab8c-bb34-4827-8215-d05b821c61b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
